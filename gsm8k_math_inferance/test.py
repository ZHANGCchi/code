# from transformers import AutoTokenizer, AutoModelForCausalLM
# from datasets import load_dataset
# from tqdm import tqdm
# import torch
# import pandas as pd
# import re
# import logging

# # è®¾ç½®æ—¥å¿—æ ¼å¼å’Œè¾“å‡ºæ–‡ä»¶
# logging.basicConfig(
#     filename="infer_log.txt",       # æ—¥å¿—ä¿å­˜çš„æ–‡ä»¶
#     filemode="a",                   # è¿½åŠ å†™å…¥
#     format="%(asctime)s - %(message)s",
#     level=logging.INFO              # æ—¥å¿—çº§åˆ«ï¼šINFO ä»¥ä¸Šçš„éƒ½ä¼šè¢«è®°å½•
# )

# # åŠ è½½æ¨¡å‹å’Œ tokenizerï¼ˆæœ¬åœ°è·¯å¾„ï¼‰
# model_id = "/root/models/qwen2.5/Qwen/Qwen2___5-Math-7B-Instruct"
# tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
# model = AutoModelForCausalLM.from_pretrained(
#     model_id,
#     device_map="auto",
#     torch_dtype=torch.float16,
#     trust_remote_code=True
# )
# model.eval()

# # åŠ è½½ GSM8K æ•°æ®
# dataset = load_dataset("json", data_files="/root/gsm8k_train.jsonl", split="train")
# print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…± {len(dataset)} æ¡æ•°æ®ã€‚")
# dataset = dataset.select(range(len(dataset)))  # å¯æ”¹æˆå…¨éƒ¨ï¼š.select(range(len(dataset)))

# # Chat æ¨¡æ¿æç¤º
# def build_messages(question):
#     return [
#         {"role": "system", "content": "Please reason step by step, and put your final answer within \\boxed{}."},
#         {"role": "user", "content": f"{question}"}
#     ]

# # æå– \boxed{...} ä¸­çš„æµ®ç‚¹ç­”æ¡ˆ
# def extract_answer(output):
#     match = re.search(r"\\boxed\{(-?\d+(?:\.\d+)?)\}", output)
#     return match.group(1) if match else None

# # å®‰å…¨æµ®ç‚¹è½¬æ¢ + å››èˆäº”å…¥
# def safe_float(s):
#     try:
#         return round(float(s), 5)
#     except:
#         return None

# results = []

# for example in tqdm(dataset, total=len(dataset)):
#     question = example["question"]
#     gt_answer_raw = example["answer"].strip().split("####")[-1].strip()
#     gt_val = safe_float(re.sub(r"[^\d\.\-]+", "", gt_answer_raw))  # åªæå–æ•°å­—éƒ¨åˆ†

#     messages = build_messages(question)
#     text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
#     inputs = tokenizer([text], return_tensors="pt").to(model.device)

#     outputs = []
#     for _ in range(10):
#         out = model.generate(**inputs, max_new_tokens=512, temperature=0.8, do_sample=True)
#         decoded = tokenizer.decode(out[0], skip_special_tokens=True)
#         pred = extract_answer(decoded)
#         outputs.append(pred)
#         logging.info("æ¨¡å‹è¾“å‡ºï¼š%s", text)

#     # è®¡ç®— pass@10
#     pass_rate = sum([safe_float(p) == gt_val for p in outputs]) / 10

#     # è¾“å‡º
#     print(f"\nğŸ¯ é—®é¢˜: {question[:60].strip()}...")
#     print(f"âœ… GT: {gt_val}, pass@10 = {pass_rate:.2f}")
#     print(f"ğŸ“Œ é¢„æµ‹: {outputs}")

#     results.append({
#         "question": question,
#         "ground_truth": gt_val,
#         "pass@10": pass_rate,
#         "predictions": outputs
#     })

# # ä¿å­˜ä¸º CSV
# df = pd.DataFrame(results)
# df.to_csv("gsm8k_pass10.csv", index=False)
# print("âœ… æ¨ç†å®Œæˆï¼Œç»“æœä¿å­˜åœ¨ gsm8k_pass10.csv")

from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
from tqdm import tqdm
import torch
import re
import logging
import csv

# ===== æ—¥å¿—è®¾ç½® =====
logging.basicConfig(
    filename="infer_log.txt",
    filemode="a",
    format="%(asctime)s - %(message)s",
    level=logging.INFO
)

# ===== åªç”¨ GPUï¼Œå¼ºåˆ¶å…¨æ¨¡å‹åœ¨ 0 å·å¡ =====
model_id = "/root/models/qwen2.5/Qwen/Qwen2___5-Math-7B-Instruct"

tokenizer = AutoTokenizer.from_pretrained(
    model_id,
    trust_remote_code=True,
    padding_side="left"
)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map={"": 0},  # å¼ºåˆ¶å…¨å¡ï¼Œé¿å…è‡ªåŠ¨åˆ†å±‚å’Œ offload
    torch_dtype=torch.float16,
    trust_remote_code=True,
)
model.eval()

# ===== æ•°æ®é›†åŠ è½½ =====
dataset = load_dataset("json", data_files="/root/gsm8k_train.jsonl", split="train")
# dataset = dataset.select(range(17))  # æµ‹è¯•ç”¨ï¼Œå®é™…æ¨ç†å…¨é‡å¯å»æ‰ select

print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…± {len(dataset)} æ¡æ•°æ®ã€‚")

batch_size = 64

with open("gsm8k_pass10.csv", "w", newline="", encoding="utf-8") as f:
    writer = csv.DictWriter(f, fieldnames=["question", "ground_truth", "pass@10", "predictions"])
    writer.writeheader()

# ===== Prompt æ„å»ºä¸ç­”æ¡ˆæå– =====
def build_messages(question):
    return [
        {"role": "system", "content": "Please reason step by step, and put your final answer within \\boxed{}."},
        {"role": "user", "content": f"{question}"}
    ]

def extract_answer(output):
    match = re.search(r"\\boxed\{(-?\d+(?:\.\d+)?)\}", output)
    return match.group(1) if match else None

def safe_float(s):
    try:
        return round(float(s), 5)
    except:
        return None

# ===== ä¸»å¾ªç¯ =====
for start in tqdm(range(0, len(dataset), batch_size)):
    batch = dataset.select(range(start, min(start + batch_size, len(dataset))))
    questions = [ex["question"] for ex in batch]
    gt_vals = [safe_float(re.sub(r"[^\d\.\-]+", "", ex["answer"].strip().split("####")[-1].strip())) for ex in batch]

    messages_batch = [build_messages(q) for q in questions]
    texts = [tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in messages_batch]
    inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}  # å…¨é‡æ˜¾å¼to GPU

    batch_outputs = [[] for _ in range(len(batch))]

    for _ in range(10):
        out = model.generate(**inputs, max_new_tokens=512, temperature=0.8, do_sample=True)
        decoded = tokenizer.batch_decode(out, skip_special_tokens=True)
        for i, output in enumerate(decoded):
            pred = extract_answer(output)
            batch_outputs[i].append(pred)
            logging.info("æ ·æœ¬ %d æ¨¡å‹è¾“å‡ºï¼š%s", start + i, output)

    for i in range(len(batch)):
        pass_rate = sum([safe_float(p) == gt_vals[i] for p in batch_outputs[i]]) / 10
        print(f"\nğŸ¯ é—®é¢˜: {questions[i][:60].strip()}...")
        print(f"âœ… GT: {gt_vals[i]}, pass@10 = {pass_rate:.2f}")
        print(f"ğŸ“Œ é¢„æµ‹: {batch_outputs[i]}")

        with open("gsm8k_pass10.csv", "a", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=["question", "ground_truth", "pass@10", "predictions"])
            writer.writerow({
                "question": questions[i],
                "ground_truth": gt_vals[i],
                "pass@10": pass_rate,
                "predictions": batch_outputs[i]
            })

print("âœ… å…¨éƒ¨æ¨ç†å®Œæˆï¼Œå¢é‡å†™å…¥å·²ä¿å­˜åˆ° gsm8k_pass10.csv")



