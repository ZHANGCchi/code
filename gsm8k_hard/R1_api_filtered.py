# -*- coding: utf-8 -*-
# APIé…ç½®
API_SECRET_KEY = "sk-cae49465bd51454ab5ead6933cdb19da"
BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"

import os
from datetime import datetime
from openai import OpenAI
from transformers import AutoTokenizer

# å¯é…ç½®çš„æ¨¡å‹åç§° â†’ ç”¨äºé€‰æ‹© tokenizer
TOKENIZER_MAP = {
    "deepseek-r1-distill-qwen-7b": "deepseek-ai/Deepseek-R1-Distill-Qwen-7B",
    "qwen2.5-math-7b-instruct": "Qwen/Qwen2.5-Math-7B-Instruct",
    "qwen2.5-7b-instruct": "Qwen/Qwen2.5-7B-Instruct"
}

def prompt_from_files(system_file="system_prompt.txt", user_file="user_prompt.txt", model_name="qwen2.5-math-7b-instruct"):
    # è¯»å–æç¤ºè¯
    with open(system_file, 'r', encoding='utf-8') as f:
        system_content = f.read().strip()
    with open(user_file, 'r', encoding='utf-8') as f:
        user_content = f.read().strip()

    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    client = OpenAI(api_key=API_SECRET_KEY, base_url=BASE_URL)

    reasoning_content = ""
    answer_content = ""
    is_answering = False

    # æ„é€ æ¶ˆæ¯
    messages = []
    if system_content:
        messages.append({"role": "system", "content": system_content})
    messages.append({"role": "user", "content": user_content})

    try:
        print("=== æ„é€ çš„æ¶ˆæ¯ ===")
        print(f"ç³»ç»Ÿè§’è‰²: {system_content[:50]}..." if len(system_content) > 50 else f"ç³»ç»Ÿè§’è‰²: {system_content}")
        print(f"ç”¨æˆ·æç¤º: {user_content[:50]}..." if len(user_content) > 50 else f"ç”¨æˆ·æç¤º: {user_content}")
        print("\nAIæ€è€ƒä¸­...")

        completion = client.chat.completions.create(
            model=model_name,
            messages=messages,
            temperature=0.7,
            stream=True
        )

        print("\n" + "=" * 20 + "æ€è€ƒè¿‡ç¨‹" + "=" * 20 + "\n")

        for chunk in completion:
            if not chunk.choices:
                continue

            delta = chunk.choices[0].delta
            delta_str = ""

            if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                delta_str = delta.reasoning_content
                reasoning_content += delta_str
            elif hasattr(delta, 'content'):
                delta_str = delta.content or ""
                answer_content += delta_str

                if not is_answering:
                    print("\n" + "=" * 20 + "å®Œæ•´å›å¤" + "=" * 20 + "\n")
                    is_answering = True

            print(delta_str, end='', flush=True)

        # è·å– tokenizer å¹¶ç»Ÿè®¡ token æ•°
        tokenizer_model = TOKENIZER_MAP.get(model_name, None)
        if tokenizer_model is None:
            print(f"\nâš ï¸ æœªçŸ¥æ¨¡å‹ {model_name}ï¼Œæ— æ³•åŠ è½½ tokenizer ç»Ÿè®¡ token")
            prompt_tokens = completion_tokens = total_tokens = -1
        else:
            tokenizer = AutoTokenizer.from_pretrained(tokenizer_model, trust_remote_code=True)
            prompt_text = system_content + "\n" + user_content
            completion_text = reasoning_content + answer_content
            prompt_tokens = len(tokenizer.encode(prompt_text))
            completion_tokens = len(tokenizer.encode(completion_text))
            total_tokens = prompt_tokens + completion_tokens

            print(f"\nğŸ“Š Token ä½¿ç”¨ç»Ÿè®¡ï¼š")
            print(f"- Prompt tokens: {prompt_tokens}")
            print(f"- Completion tokens: {completion_tokens}")
            print(f"- Total tokens: {total_tokens}")

        # ä¿å­˜æ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"ai_response_{model_name}_{timestamp}.txt"
        with open(filename, "w", encoding="utf-8") as f:
            if reasoning_content:
                f.write("=" * 20 + "æ€è€ƒè¿‡ç¨‹" + "=" * 20 + "\n")
                f.write(reasoning_content)
                f.write("\n\n")
            f.write("=" * 20 + "å®Œæ•´å›å¤" + "=" * 20 + "\n")
            f.write(answer_content)
            f.write("\n\n" + "=" * 20 + "Token ä½¿ç”¨ç»Ÿè®¡" + "=" * 20 + "\n")
            f.write(f"Prompt tokens: {prompt_tokens}\n")
            f.write(f"Completion tokens: {completion_tokens}\n")
            f.write(f"Total tokens: {total_tokens}\n")

        print(f"\nâœ… å›å¤å·²ä¿å­˜åˆ° {filename}")

    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
if __name__ == "__main__":
    prompt_from_files(model_name="deepseek-r1-distill-qwen-7b")  # å¯ä»¥ä¿®æ”¹ä¸ºå…¶ä»–æ¨¡å‹åç§°ï¼Œå¦‚ model_name="deepseek-r1-distill-qwen-7b"